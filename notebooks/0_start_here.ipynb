{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0737b18-6127-41f6-afcb-6f9acd6a555d",
   "metadata": {
    "name": "intro_md",
    "collapsed": false
   },
   "source": "# Scale Embeddings with Snowflake Notebooks on Container Runtime\n\n[Snowflake Notebooks on Container Runtime](https://docs.snowflake.com/en/user-guide/ui-snowsight/notebooks-on-spcs) are a powerful IDE option for building ML workloads at scale. Container Runtime (Public Preview) gives you a flexible container infrastructure that supports building and operationalizing a wide variety of resource-intensive ML workflows entirely within Snowflake.\n\n### What you'll be building\nNow, imagine you're a Data Scientist looking to experiment with an open source embedding model and evaluate a dataset with it before deciding to deploy it for a large batch embeddings generation (inference) job.\n\n- In the first part of this Notebook, you will first load an embedding model and generate embeddings using a GPU on a sample dataset (68K records). \n\n- In the second part, you will evaluate a sampled RAG dataset (100K records) that has various questions and associated context chunks (\"labels\"). After evaluation, you will deploy the embedding model and perform inference on the full RAG dataset (10M context chunks).\n\n### :exclamation: IMPORTANT :exclamation:\nMake sure to go to `Edit compute settings > External access` (Dropdown next to the the `start` button) and enable all access integrations that were created in the `setup.sql` step."
  },
  {
   "cell_type": "markdown",
   "id": "c14e3f2d-8b96-49ab-ba7b-f7c81c07043f",
   "metadata": {
    "name": "import_md",
    "collapsed": false
   },
   "source": "First, we will import some basic libraries and get the Snowflake session object. We will also install some libraries that we'll need."
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "import pandas as pd\nimport numpy as np\nsession = get_active_session()\n\n# Add a query tag to the session. This helps with debugging and performance monitoring.\nsession.query_tag = {\"origin\":\"sf_sit-is\", \"name\":\"cr_notebooks_embeddings\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"is_quickstart\":1, \"source\":\"notebook\"}}\n\n# Set session context \nsession.use_role(\"EMBEDDING_MODEL_HOL_USER\") \n\n# Print the current role, warehouse, and database/schema\nprint(f\"role: {session.get_current_role()} | WH: {session.get_current_warehouse()} | DB.SCHEMA: {session.get_fully_qualified_current_schema()}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0b6dfe40-c645-47fa-8e46-1ce3aca1c546",
   "metadata": {
    "language": "python",
    "name": "pip_installs",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "! pip install snowflake-ml-python==1.6.3 sentence-transformers --quiet",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a052a1be-2555-4788-9c4f-6eb5f4411d4a",
   "metadata": {
    "name": "part_1_md",
    "collapsed": false
   },
   "source": "## PART 1: Getting started with embeddings\n\nLet's load an open source embedding model using `SentenceTransformer()` and show how we can generate embeddings on a sample sentence dataset and store those embeddings as a `VectorType()` in a Snowflake table."
  },
  {
   "cell_type": "code",
   "id": "ac9d9669-a27f-4dea-8cc7-d58eb5ae171b",
   "metadata": {
    "language": "python",
    "name": "load_model",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from sentence_transformers import SentenceTransformer\n\n# Take an example sentence transformer from HF\nembed_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',\n                                  trust_remote_code=True,\n                                  device='cuda')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b6d83d29-9382-4005-bb53-6ef22622fff9",
   "metadata": {
    "name": "load_sst2_md",
    "collapsed": false
   },
   "source": "Let's load a sample sentence dataset called `SST2`."
  },
  {
   "cell_type": "code",
   "id": "a6be8bd6-f892-4445-8e91-4e6f1efb51bc",
   "metadata": {
    "language": "python",
    "name": "load_sst2",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from datasets import load_dataset\n\n# Load SST2 dataset and rename columns\nsst2_data = load_dataset('sst2')\ndf_pd = pd.DataFrame(sst2_data['train'])\ndf_pd = df_pd.rename(columns={'idx': 'IDX', 'sentence': 'SENTENCE', 'label': 'LABEL'})\n\ndf_pd.shape",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f57bdf7-68f0-4090-ad10-9deda6f31b01",
   "metadata": {
    "name": "embeddings_sst2_md",
    "collapsed": false
   },
   "source": "Now, we're ready to generate embeddings on this dataset."
  },
  {
   "cell_type": "code",
   "id": "a619576a-c24b-4b9a-b62f-9cd82a2831e8",
   "metadata": {
    "language": "python",
    "name": "embeddings_sst2",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Generate embeddings \nembeddings = embed_model.encode(df_pd[\"SENTENCE\"].to_list(), \n                          show_progress_bar=True)\ndf_pd['EMBEDDING'] = embeddings.tolist()\ndf_pd.head()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0791ade4-2438-437c-b98d-3d51d1cf0baf",
   "metadata": {
    "name": "create_sdf_md",
    "collapsed": false
   },
   "source": "We will now create a Snowpark DataFrame to store the results."
  },
  {
   "cell_type": "code",
   "id": "a17df1bd-7c97-4e40-b5c9-e1407ea9b7c5",
   "metadata": {
    "language": "python",
    "name": "create_sdf",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df_sdf = session.create_dataframe(df_pd)\ndf_sdf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "54a8b2c8-fda4-4d00-b093-385db1f796fb",
   "metadata": {
    "name": "cast_embed_md",
    "collapsed": false
   },
   "source": "Let's cast the embeddings into Snowflake's `VectorType()`."
  },
  {
   "cell_type": "code",
   "id": "7ac4d2e2-9589-4c6a-85d9-505402cd4784",
   "metadata": {
    "language": "python",
    "name": "cast_embed",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.types import VectorType\n\ndf_sdf = df_sdf.with_column('EMBEDDING', df_sdf['EMBEDDING'].cast(VectorType(float, 384)))\ndf_sdf.limit(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9bb7fa02-a150-4fd0-aa2a-ff28c5fca690",
   "metadata": {
    "name": "write_table_md",
    "collapsed": false
   },
   "source": "Now, we're ready to write the results into a Snowflake table. "
  },
  {
   "cell_type": "code",
   "id": "f0892e19-be43-494d-a425-3b5805daf3d7",
   "metadata": {
    "language": "python",
    "name": "write_table",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df_sdf[['SENTENCE',\n        'EMBEDDING']].write.save_as_table('SST2_EMBEDDINGS', mode=\"overwrite\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29bf8a61-5098-40e2-9e9a-82b203534aae",
   "metadata": {
    "name": "select_table_md",
    "collapsed": false
   },
   "source": "Let's take a look at what this table looks like."
  },
  {
   "cell_type": "code",
   "id": "4a3a9b6d-7f22-4546-89ec-a710e25d67e4",
   "metadata": {
    "language": "sql",
    "name": "select_table",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM SST2_EMBEDDINGS \nLIMIT 5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ab8fe13-adcd-444f-ba59-b9bc3457c1f1",
   "metadata": {
    "name": "part_2_md",
    "collapsed": false
   },
   "source": "## PART 2: Evaluate a RAG dataset and perform large scale batch inference\n\nLet's now experiement with generating embeddings for a RAG solution. We will calculate embeddings for the `CONTEXT` chunks we have and a sample set of `QUESTIONS`. Basically, we want to evaluate whether the correct chunk is being pulled for each question.\n\nIf we're happy with the accuracy, we will go ahead and deploy the model to generate embeddings at scale on a larger dataset."
  },
  {
   "cell_type": "markdown",
   "id": "704050ea-4b10-4580-bc13-aee449baab8e",
   "metadata": {
    "name": "load_rag_data_md",
    "collapsed": false
   },
   "source": "Let's load an open source RAG dataset from HuggingFace and oversample to create a larger dataset (~10M records)."
  },
  {
   "cell_type": "code",
   "id": "e15d096b-8424-4dc6-a985-ebebc980c7c2",
   "metadata": {
    "language": "python",
    "name": "load_rag_data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# For more info: https://huggingface.co/datasets/neural-bridge/rag-dataset-12000\nds = load_dataset(\"neural-bridge/rag-dataset-12000\")\n\ndf_pd_rag = pd.DataFrame(ds['train'])\ndf_pd_rag = df_pd_rag.rename(columns={'context': 'CONTEXT'})\n\n# Oversample to create a larger dataset\nnewdf = df_pd_rag.loc[np.repeat(df_pd_rag.index, 100)].reset_index(drop=True)\n\nfor _ in range(10):\n    print(f'{_+1}0% complete')\n    session.write_pandas(newdf, \"RAG_DATASET_10M\", \n                         auto_create_table=True, \n                         overwrite=False,\n                         chunk_size=10000)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5d1b8bbb-5f05-4756-8de2-654cf5daf979",
   "metadata": {
    "name": "sample_rag_data_md",
    "collapsed": false
   },
   "source": "Since we're just experimenting and evaluating at this stage, let's sample this dataset to 100k records."
  },
  {
   "cell_type": "code",
   "id": "d2ec66d9-11e0-4a80-8c1c-a9d654dc49f7",
   "metadata": {
    "language": "python",
    "name": "sample_rag_data",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "df_rag_sample = session.table('RAG_DATASET_10M').limit(100000).to_pandas()\ndf_rag_sample = df_rag_sample.rename(columns={'question': 'QUESTION'})",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "46a4e7ce-e01c-4704-9b82-c47f8d367eac",
   "metadata": {
    "name": "context_embed_md",
    "collapsed": false
   },
   "source": "Now, we can go ahead and generate embeddings on the `CONTEXT` chunks in our dataset."
  },
  {
   "cell_type": "code",
   "id": "0c3b05d5-f580-4eb8-bc0f-cd45eb0ff3ca",
   "metadata": {
    "language": "python",
    "name": "context_embed",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "context_sample_list = df_rag_sample[\"CONTEXT\"].to_list()\n\ncontext_embeddings = embed_model.encode(context_sample_list,\n                                        show_progress_bar=True)\n\ndf_rag_sample['CONTEXT_EMBEDDINGS'] = context_embeddings.tolist()\ndf_rag_sample.head()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "02671b8c-a5e1-4773-9663-cf464d1084f4",
   "metadata": {
    "name": "sample_ques_md",
    "collapsed": false
   },
   "source": "Let's select a sample of 1000 questions to evaluate and generate embeddings for those as well."
  },
  {
   "cell_type": "code",
   "id": "f633b7c1-2f6e-408d-a3ed-26611dfb5fb7",
   "metadata": {
    "language": "python",
    "name": "sample_ques_embed",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df_rag_sample_q = pd.DataFrame(df_rag_sample[[\"QUESTION\", \"CONTEXT\"]].sample(1000))\ndf_rag_sample_q = df_rag_sample_q.rename(columns={\"CONTEXT\": \"LABELED_CONTEXT\"})\n\nquestion_sample_list = df_rag_sample_q[\"QUESTION\"].to_list()\n\nquestion_embeddings = embed_model.encode(question_sample_list, \n                                show_progress_bar=True)\n\ndf_rag_sample_q['QUESTION_EMBEDDINGS'] = question_embeddings.tolist()\ndf_rag_sample_q.head()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a21dc9e-b720-4f92-bb16-64dd2dea8447",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "We will want to keep track of the correct `CONTEXT` per `QUESTION` as well."
  },
  {
   "cell_type": "code",
   "id": "81af7774-6a22-4c40-96ba-0f32cae0a63b",
   "metadata": {
    "language": "python",
    "name": "sample_ques_list",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "question_labels_list = df_rag_sample_q[\"LABELED_CONTEXT\"].to_list()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e16417cb-9fce-4e2e-b7ab-6b59ae91fce3",
   "metadata": {
    "name": "semantic_search_md",
    "collapsed": false
   },
   "source": "Finally, we can evaluate our embedding model on our chosen sample of questions to generate a relevance score.\n\nWe'll be using the `util.semantic_search()` function from `sentence_transformers` to select the top `CONTEXT` per `QUESTION` to see whether we pick the correct `CONTEXT` chunk."
  },
  {
   "cell_type": "code",
   "id": "bafd25af-7d23-40b3-be95-3d211a6896d2",
   "metadata": {
    "language": "python",
    "name": "semantic_search",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from sentence_transformers import util\n\nhits = util.semantic_search(question_embeddings, context_embeddings, top_k=1)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a20de7db-3169-41d4-bd42-f1f6c7fc483a",
   "metadata": {
    "language": "python",
    "name": "format_results",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "aDict = {}\nfor n, item in enumerate(hits):\n    item[0][\"QUESTION\"] = question_sample_list[n]\n    item[0][\"LABELED_CONTEXT\"] = question_labels_list[n]\n    aDict[n] = item[0]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b6f9615-e3c4-42b5-a65b-7a0aafb1434c",
   "metadata": {
    "language": "python",
    "name": "results_to_df",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "results_df = pd.DataFrame.from_dict(aDict, orient='index')\nresults_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1201d78f-aad4-41bb-b708-83f4ce9614de",
   "metadata": {
    "name": "merge_results_orig_data_md",
    "collapsed": false
   },
   "source": "Join `corpus_id` with the original dataset to get the CONTEXT field from the original dataset."
  },
  {
   "cell_type": "code",
   "id": "551bf7b1-a108-4633-9def-c8d43b83fbd3",
   "metadata": {
    "language": "python",
    "name": "merge_results_orig_data",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "merged_df = pd.merge(df_rag_sample, results_df, \n                      left_index=True, \n                      right_on=['corpus_id'], how='inner')\nmerged_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1eed6c6a-a51f-4e46-b568-fe39ca8dccd6",
   "metadata": {
    "language": "python",
    "name": "results",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "correct_results = merged_df[merged_df['CONTEXT']==merged_df['LABELED_CONTEXT']].count()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2c71b92-b6fc-4f79-be55-8c7408102eed",
   "metadata": {
    "name": "accuracy_md",
    "collapsed": false
   },
   "source": "We compute the accuracy value now to see how many times the correct `CONTEXT` chunk was pulled for each of our sample questions."
  },
  {
   "cell_type": "code",
   "id": "c89aa3b2-c152-423d-a97f-a2d91b05034f",
   "metadata": {
    "language": "python",
    "name": "accuracy",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "accuracy = correct_results.values[0]/merged_df.count()['LABELED_CONTEXT'] * 100\nf'''Percent accuracy: {accuracy:.2f}%'''",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "615ee0bc-fd2a-4b0c-828a-4e4c820f8e6f",
   "metadata": {
    "name": "proceed_md",
    "collapsed": false
   },
   "source": "The accuracy looks good for us to proceed and deploy the embedding model for perform a batch inference job on the full ~10M `CONTEXT` chunks now."
  },
  {
   "cell_type": "markdown",
   "id": "67110702-fecf-4498-b61f-476bccdc179f",
   "metadata": {
    "name": "deploy_model_md",
    "collapsed": false
   },
   "source": "In order to deploy the model, we will be using [Snowflake Model Registry](https://docs.snowflake.com/developer-guide/snowflake-ml/model-registry/overview?utm_cta=snowpark-dg-hero-card).\n\nThe Snowflake Model Registry lets you securely manage models and their metadata in Snowflake, regardless of origin. The model registry stores machine learning models as first-class schema-level objects in Snowflake so they can easily be found and used by others in your organization. You can create registries and store models in them using Python classes in the Snowpark ML library. Models can have multiple versions, and you can designate a version as the default.\n\nAfter you have stored a model, you can invoke its methods (equivalent to functions or stored procedures) to perform model operations, such as inference\n\nFirst, let's create a `Registry` instance."
  },
  {
   "cell_type": "code",
   "id": "4fc3beaa-b17a-4930-802a-a75426105566",
   "metadata": {
    "language": "python",
    "name": "model_registry",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\n\n# Create Model Registry\nreg = Registry(\n    session=session, \n    database_name=session.get_current_database(), \n    schema_name=session.get_current_schema()\n    )\n\nreg",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4da2ab76-f2cd-4016-a3ba-c6b9f6d05727",
   "metadata": {
    "name": "model_sig_md",
    "collapsed": false
   },
   "source": "We need to specify the Model Signature in order to log this model."
  },
  {
   "cell_type": "code",
   "id": "28b2a475-cc49-4383-931f-fb531340547d",
   "metadata": {
    "language": "python",
    "name": "model_sig",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.model.model_signature import FeatureSpec, DataType, ModelSignature\n\n# In this example the output column will be called CONTEXT_EMBEDDING \n# and have a shape of (384,)\nmodel_sig = ModelSignature(\n                  inputs=[\n                          FeatureSpec(dtype=DataType.STRING, name='CONTEXT')\n                      ],\n                      outputs=[\n                          FeatureSpec(dtype=DataType.DOUBLE, name='CONTEXT_EMBEDDING', shape=(384,))\n                      ]\n                  )\n\nmodel_sig",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "88f995d8-ef9a-4ab3-99bb-f26832c77063",
   "metadata": {
    "name": "log_model_md",
    "collapsed": false
   },
   "source": "Now, we can log the model."
  },
  {
   "cell_type": "code",
   "id": "f04f7599-9300-4bcd-b693-3cf8d24222b8",
   "metadata": {
    "language": "python",
    "name": "log_model",
    "codeCollapsed": false,
    "collapsed": true
   },
   "outputs": [],
   "source": "# Logging the sentence_transformers model, using pip requirements, deployment against the gpu\nmv = reg.log_model(embed_model,\n                   model_name=\"sentence_transformer_minilm\",\n                   version_name='v1',\n                   pip_requirements=[\"sentence-transformers\", \"torch\", \"transformers\"], \n                   signatures = {'encode': model_sig},\n                   options = {\"cuda_version\": \"11.8\"},\n                   comment = \"Model artifact associated with deployment against GPU\"\n                  )",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0c5e1f95-7965-484b-a23f-03128508c8b1",
   "metadata": {
    "name": "check_model_md",
    "collapsed": false
   },
   "source": "Because you're using pip requirements, this model will be deployed as a service on SPCS using the new Model Serving functionality. it will not run on the warehouse. if you logged it using conda requirments, it would run on the warehouse also.\n\nLet's make sure the model got logged."
  },
  {
   "cell_type": "code",
   "id": "1512c16c-42c2-4a1c-aca8-a6237f7085fc",
   "metadata": {
    "language": "python",
    "name": "show_model",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "reg.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8cb9bfcf-1f1b-4535-8856-ec1c37e63d14",
   "metadata": {
    "name": "get_model_md",
    "collapsed": false
   },
   "source": "We can also get our reference to the model using `get_model()` and see the associate `functions()` we can call with our model."
  },
  {
   "cell_type": "code",
   "id": "c78af8e6-db24-4d1b-b1eb-ec318585939d",
   "metadata": {
    "language": "python",
    "name": "get_model",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "mv = reg.get_model('sentence_transformer_minilm').version('v1')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a673fd10-546f-4d52-a721-07d908b05877",
   "metadata": {
    "language": "python",
    "name": "show_model_func",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "mv.show_functions()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57545875-4e5d-4b69-aab7-4311f1e048e4",
   "metadata": {
    "name": "create_service_md",
    "collapsed": false
   },
   "source": "Now we need to create a service that will host our model on GPUs. Let's make sure our service can use as many GPUs as we have access to outside of the single GPU that our Notebook is using. During `setup.sql` we set 4 GPUs (nodes) to be the max capacity, so we can dedicate 3 to the inference service.\n\n**Note:** This step takes some time and will print log statements below."
  },
  {
   "cell_type": "code",
   "id": "645a2df2-3989-42a1-ac6c-8cdc9a6fe43d",
   "metadata": {
    "language": "python",
    "name": "create_service",
    "codeCollapsed": false,
    "collapsed": true
   },
   "outputs": [],
   "source": "#Create the service and call it:\nmv.create_service(service_name=\"minilm_gpu_service\",\n                  service_compute_pool=\"GPU_NV_S_COMPUTE_POOL\",\n                  image_repo=f\"{session.get_current_database()}.{session.get_current_schema()}.my_inference_images\",\n                  ingress_enabled=True,\n                  build_external_access_integration=\"ALLOW_ALL_INTEGRATION\", #allows access to pypi to build\n                  gpu_requests = \"1\", #max number of GPUs needs to match GPU nodes in the compute pool Small --> 1 instance\n                  max_instances = 3\n                )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c910d55e-16e2-4352-bea2-929ecfa97712",
   "metadata": {
    "language": "sql",
    "name": "show_services",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- Keep refreshing this to check whether status = RUNNING\nSHOW SERVICES IN COMPUTE POOL GPU_NV_S_COMPUTE_POOL;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1f1b0e5-993b-4cb5-a1c4-f689e5e2c921",
   "metadata": {
    "name": "load_full_data_md",
    "collapsed": false
   },
   "source": "Once the inference service is ready, let's load our full 10M dataset."
  },
  {
   "cell_type": "code",
   "id": "6cd0c91f-0c4b-47f6-82ba-ad25f8067dbe",
   "metadata": {
    "language": "python",
    "name": "load_full_data",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "full_rag_dataset = session.table('RAG_DATASET_10M')\nfull_rag_dataset",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b7915bdb-6f42-454d-894c-41b52ae6383c",
   "metadata": {
    "name": "inference_job_md",
    "collapsed": false
   },
   "source": "Now, let's run the inference job and save the embeddings to a Snowflake table. \n\n**Note:** This step will also take some time and will complete async, so you can monitor the underlying query under `Monitoring > Query History`.\n\nOnce, it's completed, you should be able to see the table within the `EMBEDDING_MODEL_QUICKSTART_DB` that was created to store the embeddings."
  },
  {
   "cell_type": "code",
   "id": "c049350e-1476-4fe0-ba5e-202fa8e865aa",
   "metadata": {
    "language": "python",
    "name": "inference_job",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import functions as F\nfrom snowflake.snowpark import types as T\n\noutput_10M = mv.run(full_rag_dataset, function_name = 'encode', service_name = 'minilm_gpu_service')\noutput_10M = output_10M.with_column('CONTEXT_EMBEDDING', F.col('CONTEXT_EMBEDDING').cast(T.VectorType(float, 384)))\\\n                       .select('CONTEXT', 'CONTEXT_EMBEDDING')\n\n# We can now run an async job \noutput_10M.write.mode('overwrite').save_as_table('RAG_DATASET_10M_OUTPUT', block = False)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d5e07716-3dc2-43b3-b4f6-8d7d653a3a86",
   "metadata": {
    "name": "conclusion",
    "collapsed": false
   },
   "source": "## Conclusion\n\nWithin this Notebook, you loaded an embedding model, generated embeddings using a GPU, evaluated a dataset for a RAG solution, deployed the embedding model, performed large scale batch inference, and saved results to a Snowflake table without a lot of complex infrastructure setup and management."
  }
 ]
}